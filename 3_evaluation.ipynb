{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackGraymer/Advanced-GenAI/blob/main/3_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def7af22",
      "metadata": {
        "id": "def7af22"
      },
      "source": [
        "# Advanced Generative Artificial Intelligence\n",
        "**Project - Designing a RAG-Based Q&A System for News Retrieval**\n",
        "\n",
        "**Authors:** Vsevolod Mironov, Pascal Küng, Alvaro Cervan (Group 5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a47b6c8",
      "metadata": {},
      "source": [
        "# Step 3 Evaluation – Assessing answer quality through both automated and human evaluation\n",
        "\n",
        "**Contribution:** Vsevolod Mironov, Pascal Küng, Alvaro Cervan\n",
        "\n",
        "**Goal of this step:** Students will assess the quality of answers produced by their top-performing RAG pipeline. This involves applying the pipeline to benchmark questions, comparing its responses to ground truth answers, and evaluating performance using both automated metrics and human judgment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73008db1",
      "metadata": {},
      "source": [
        "### **Objective**\n",
        "Evaluate the quality of answers generated by the best RAG pipeline using both automated metrics and human judgment.\n",
        "\n",
        "---\n",
        "\n",
        "### **Workflow**\n",
        "\n",
        "1. **Run the Best RAG Pipeline**\n",
        "\t- Apply the developed RAG pipeline to the benchmark question set to generate answers.\n",
        "\n",
        "2. **Automated Metrics Calculation**\n",
        "\t- **Semantic Exact Match:** Assess if generated answers semantically match ground truth using embeddings or semantic models.\n",
        "\t- **Semantic F1 Score:** Tokenize answers and compute precision, recall, and F1 based on semantic similarity.\n",
        "\t- **BLEU/ROUGE:** Measure N-gram or sequence overlap between generated and ground truth answers.\n",
        "\t- **Record Results:** Store all metric scores in a structured format for comparison.\n",
        "\n",
        "3. **Human Evaluation**\n",
        "\t- **Criteria:** Evaluate each answer for:\n",
        "\t  - *Relevance* (alignment with the query)\n",
        "\t  - *Correctness* (accuracy vs. ground truth)\n",
        "\t  - *Clarity* (understandability)\n",
        "\t- **Rating Scale:** Use a 1–5 scale for each criterion.\n",
        "\t- **Analysis:** Provide a brief written summary of human evaluation findings.\n",
        "\n",
        "4. **Report and Presentation**\n",
        "\t- Present both automated and human evaluation results using tables, charts, or other concise visualizations within the notebook.\n",
        "\n",
        "#### Visual Representation of the Evaluation Pipeline\n",
        "![Pipeline Overview](Evaluation_Workflow.svg)\n",
        "\n",
        "---\n",
        "\n",
        "**Summary Table Example:**\n",
        "\n",
        "| Question | Automated Metrics (F1, BLEU, ROUGE) | Human Relevance | Human Correctness | Human Clarity | Comments |\n",
        "|----------|-------------------------------------|-----------------|-------------------|--------------|----------|\n",
        "| Q1       | ...                                 | ...             | ...               | ...          | ...      |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80964a2",
      "metadata": {},
      "source": [
        "# 1.0 Loading Data and functions from previous stage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77638683",
      "metadata": {},
      "source": [
        "## 1.1 Setup of the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c962e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import asyncio\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pprint\n",
        "import pickle\n",
        "import faiss\n",
        "from typing import Optional, List\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from FlagEmbedding import FlagReranker\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4121ec55",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the seed for consistent results\n",
        "seed_value = 2138247234\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f7ff00",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81620a69",
      "metadata": {},
      "outputs": [],
      "source": [
        "base_folder = '/content/drive/MyDrive/AdvGenAI'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec9f9e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell if working locally\n",
        "df = pd.read_csv('data/Stage2-02-chunked-dataset.csv')\n",
        "with open('data/Stage2-08-q-a-file-with-relevancy.pkl', 'rb') as f:\n",
        "\tQ_A_ground_thruth_relevancy_dict = pickle.load(f)\n",
        "\n",
        "\tfilename = 'data/Stage3-01-precalc-retrieved-chunks.pkl'\n",
        "\twith open(filename, 'rb') as f:\n",
        "\t\tprecalc_retrieved_chunks = pickle.load(f)\n",
        "\tprint(f\"Dictionary loaded from {filename}:\")\n",
        "\n",
        "# load files in local computer and api from data .env\n",
        "from dotenv import dotenv_values\n",
        "\n",
        "env_vars = dotenv_values('data/.env')\n",
        "OPENAI_API_KEY = env_vars.get('OPENAI_API_KEY', None)\n",
        "print(f\"Loaded OPENAI_API_KEY: {'***' if OPENAI_API_KEY else 'Not found'}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
