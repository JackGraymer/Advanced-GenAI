{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JackGraymer/Advanced-GenAI/blob/main/1_data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "def7af22"
   },
   "source": [
    "# Advanced Generative Artificial Intelligence\n",
    "**Project - Designing a RAG-Based Q&A System for News Retrieval**\n",
    "\n",
    "**Authors:** Vsevolod Mironov, Pascal Küng, Alvaro Cervan (Group 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "6PwMu1LYwuKN"
   },
   "source": [
    "# Step 1 - Data preparation\n",
    "\n",
    "**Contribution:** Vsevolod Mironov, Pascal Küng, Alvaro Cervan\n",
    "\n",
    "**Goal of this step:** To create a clean and well-structured multilingual dataset of news articles with enriched metadata, optimized for efficient indexing and retrieval in a future RAG-based system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "X3X9lADadwyy"
   },
   "source": [
    "# Loading, Parsing, and Cleaning HTML Files (5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "m6Wz1IFixGYf"
   },
   "source": [
    "## 1. Setup of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "s5Hs0cyezAsU"
   },
   "source": [
    "Below the necessary libraries are installed and loaded into the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p474BKFnkqrh",
    "outputId": "9a5967fc-9934-4451-df0b-1054c8b48587"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3DBydUBor8w",
    "outputId": "853b9f1a-016b-49d2-97d2-b6fd56d20d93"
   },
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcpJXBF-pNfk",
    "outputId": "021dfdd9-17d0-4abc-ea4e-b7eff7635413"
   },
   "outputs": [],
   "source": [
    "!pip install -q docling==2.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "qaC38lmUpkOH"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Comment\n",
    "import docling\n",
    "from docling.document_converter import DocumentConverter, InputFormat, HTMLFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.pipeline_options import PipelineOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndWv0kaFxtGy",
    "outputId": "72bdc526-7175-425c-c209-204e30268769"
   },
   "outputs": [],
   "source": [
    "!pip install -q beautifulsoup4==4.13.4\n",
    "!pip install -q docling==2.31.0\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import docling\n",
    "from docling.document_converter import DocumentConverter, InputFormat, HTMLFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import AcceleratorDevice, AcceleratorOptions\n",
    "from docling.datamodel.pipeline_options import PipelineOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "id": "WzYPTuTAwt1B"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "id": "Ky79voSFYfdF"
   },
   "outputs": [],
   "source": [
    "# Set the seed for consistent results\n",
    "seed_value = 2138247234\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "42G89w-nzGxB"
   },
   "source": [
    "Below we mount a shared Google Drive folder as a data storage and define the base path of the folder that will be used in the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Qsc7aoTzPmG",
    "outputId": "4a0b5f35-d99d-4d81-a3aa-8db462bd6cdd"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "id": "2HLrG6dV0Xer"
   },
   "outputs": [],
   "source": [
    "base_folder = '/content/drive/MyDrive/AdvGenAI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "rho9pmTF0KVQ"
   },
   "source": [
    "## 2. Loading the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "KcLDISALPz7C"
   },
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "6RXR7U5P1gSc"
   },
   "source": [
    "We go through the subdirectories inside the data-folder. Inside those folders the individual html-files will be read and the content will be saved together with the information of the file-name and the path of the file (to store in which subfolder it was located)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "UYlAyNfZ0Uha"
   },
   "outputs": [],
   "source": [
    "# Definition of data folder\n",
    "data_folder = os.path.join(base_folder, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFdLmMrj0PRy",
    "outputId": "c9c1340b-44e2-4ab1-e1c1-5a52498db818"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# List to hold the dictionaries\n",
    "data = []\n",
    "\n",
    "# Walk through all directories and subdirectories\n",
    "for root, dirs, files in os.walk(data_folder):\n",
    "    for file in files:\n",
    "        if file.endswith('.html'):\n",
    "            file_path = os.path.join(root, file)\n",
    "\n",
    "            # Read the content of the HTML file\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            # Add a dictionary to the list\n",
    "            data.append({\n",
    "                'folder_path': root,\n",
    "                'file_name': file,\n",
    "                'full_path': file_path,\n",
    "                'html_content': content\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally save DataFrame, e.g. to CSV or pickle for later use\n",
    "# df.to_csv('html_files_content.csv', index=False)\n",
    "# df.to_pickle('html_files_content.pkl')\n",
    "\n",
    "# Show first rows to verify\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "d4XOtgzy8sFz",
    "outputId": "a3038a89-7b6d-4dd4-b418-5e529a2f71be"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oH0Odvr94C21",
    "outputId": "a2a60e43-ff1e-4aab-c4ee-dbe246f165b2"
   },
   "outputs": [],
   "source": [
    "print(df.iloc[10:100, 2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PH_xAa8L8Ao7",
    "outputId": "21fe32e8-306b-4fa8-9530-756eb593ea99"
   },
   "outputs": [],
   "source": [
    "print(df.iloc[-100:-1, 2].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "5rgD4Gi8P3m7"
   },
   "source": [
    "### Checking completeness of loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "1dBquybd84P9"
   },
   "source": [
    "**Number of files**\n",
    "\n",
    "Below we compare the number of documents collected by the function into the Dataframe with a selection of all files in the data folder.\n",
    "\n",
    "In the check 3 files were discovered that were not part of the dataframe. After inspection it was discovered that those are `.DS_Store`file, for which it makes sense that they were not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yf7OGnHT9Bph",
    "outputId": "25a1e0c8-977e-47f2-9c21-8a5ed76ab7ef"
   },
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "print(f\"Number of files in the DataFrame: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnA43RVY4_gi",
    "outputId": "e34a63a4-1e6d-4889-f49c-6d00c53c5707"
   },
   "outputs": [],
   "source": [
    "# Files in Data folder\n",
    "print(f\"Number of files in the data folder:\")\n",
    "!find \"$data_folder\" -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iu2k2QXt9tBE",
    "outputId": "e035af9d-705e-4b0f-fd34-c90b13ec3125"
   },
   "outputs": [],
   "source": [
    "!find \"$data_folder\" -type f | sort > folder_files.txt\n",
    "df['full_path'].sort_values().to_csv('df_files.txt', index=False, header=False)\n",
    "!sort folder_files.txt -o folder_files.txt\n",
    "!sort df_files.txt -o df_files.txt\n",
    "!comm -23 folder_files.txt df_files.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "RQEgww_SQA_p"
   },
   "source": [
    "**Checking for empty files**\n",
    "\n",
    "Below we print out the rows of the dataframe with empty contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "bfc5iHT4PSCQ",
    "outputId": "ecd6b89b-2bda-4f5b-f4bd-d58b9e3e5490"
   },
   "outputs": [],
   "source": [
    "df[df['html_content'].isna()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 574
    },
    "id": "AZh08Z-wLvgX",
    "outputId": "4aaa3f0b-431b-4d18-ad1d-dbd2d80ff878"
   },
   "outputs": [],
   "source": [
    "print(f\"Files with no content: {len(df[df['html_content']==''])}\")\n",
    "df[df['html_content']==\"\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vSrZtTdPvA_",
    "outputId": "d5a31804-51e3-45df-e4e8-3ec70d4add46"
   },
   "outputs": [],
   "source": [
    "print(df.loc[df['html_content']==\"\", \"full_path\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "id": "HNCX46D1QK3K"
   },
   "source": [
    "After checking the files in the original data source we concluded that those files were empty files and therefore it was not a problem in the process of the data loading. We therefore exclude those rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QWoARfKdDZPo",
    "outputId": "a8520f54-4783-4b42-c583-200db0feea32"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "4tlInvByQp2P"
   },
   "outputs": [],
   "source": [
    "df = df[~df['html_content'].isna()].copy()\n",
    "df = df[~(df['html_content']==\"\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRQ1FLcKDe79",
    "outputId": "c9bef402-3e82-43d9-d317-126d7b0883af"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L492xaK2D57O",
    "outputId": "f8a7f40f-8753-4b39-b992-4b30ef43edef"
   },
   "outputs": [],
   "source": [
    "print(df.iloc[:,3].sort_values().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1WvxdRZFI3u-",
    "outputId": "a9e356ad-a266-4fdd-f995-b8e1fc77ae9b"
   },
   "outputs": [],
   "source": [
    "print(df.shape)  # Output: (rows, columns)\n",
    "print(df.columns)  # List all column names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "c2R6lFCBQskm"
   },
   "source": [
    "### Saving the data to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "D4E5HIFdLcr8"
   },
   "outputs": [],
   "source": [
    "# Saving the Dataframe to the Google Drive storage\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-01-raw-data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "nc3vH25p7c1z"
   },
   "source": [
    "## 3. Parsing and cleaning the HTML files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "id": "xiT3FLavQxW0"
   },
   "source": [
    "### Loading the data from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "id": "eb-gX82-M-wz"
   },
   "outputs": [],
   "source": [
    "# Load csv from Google Drive Storage to Dataframe\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-01-raw-data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZkYlSFbjNRLe",
    "outputId": "c43881b8-5732-433b-8ecb-ef00db5bfc9a"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 300)\n",
    "df[[\"html_content\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "HDeelq_WOBFt"
   },
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "QNe0eYn1OXhI"
   },
   "source": [
    "#### Definition of cleaning function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "4fp4k5TmNcZP"
   },
   "source": [
    "Below a function is defined to clean the stored strings of the html-files using `BeautifulSoup`. It extracts the title and main texts of the documents while removing various elements that are not of interest for the further analysis (for example style and navigation elements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "id": "IKtC45X5ILgR"
   },
   "outputs": [],
   "source": [
    "def clean_html(html_content):\n",
    "    from bs4 import BeautifulSoup, Comment\n",
    "    import re\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Title extraction\n",
    "    title = soup.title.get_text(strip=True) if soup.title else ''\n",
    "\n",
    "    # Remove unwanted elements\n",
    "    for el in soup(['script', 'style', 'header', 'footer', 'nav', 'iframe', 'meta', 'link']):\n",
    "        el.decompose()\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    # Replace <br> with newline\n",
    "    for br in soup.find_all(\"<br/>\"):\n",
    "        br.replace_with(\"\\n\")\n",
    "\n",
    "    # Get the content from body if exists\n",
    "    content = soup.body or soup\n",
    "\n",
    "    # get_text with separator\n",
    "    clean_text = content.get_text(separator='\\n\\n').strip()\n",
    "\n",
    "    # Post-process: Collapse excessive blank lines\n",
    "    clean_text = re.sub(r'\\n{3,}', '\\n\\n', clean_text)\n",
    "\n",
    "    return title, clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "Y3ZvEHknOct9"
   },
   "source": [
    "#### Application of cleaning function on subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {
    "id": "AL4tSWLOK5z4"
   },
   "source": [
    "**Add description (trying it out with a subset), the function uses `\\n\\n` for separation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "id": "rJGCm96n_dBG"
   },
   "outputs": [],
   "source": [
    "# create a subset of the dataframe for testing\n",
    "df_test = df.sample(n=5).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "id": "_XylXkAy_WCM"
   },
   "outputs": [],
   "source": [
    "df_test['title'], df_test['clean_content'] = zip(*df_test['html_content'].apply(clean_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "1i_Ocpp2_b8y",
    "outputId": "c1fd532c-d5c2-415b-d5ec-375537ef702f"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df_test[['html_content', 'title', 'clean_content']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "nrRhEX1nc76v"
   },
   "source": [
    "Below we print out the HTML and the cleaned content for each document for comparison. For the cleaned content the double newlines are replaced by `\\n---PARAGRAPH BREAK---\\n` for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage/image.imageformat.text50percent.167220564.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage_82956239/image.imageformat.text50percent.342924418.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/en/news-and-events/eth-news/news/2021/06/were-all-greeks/_jcr_content/news_content/textimage_1347267382/image.imageformat.text50percent.2137756140.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "id": "681jaWaXCWrR",
    "outputId": "abb2de93-dc35-4348-8381-1b7ad84cd623"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "for idx, row in df_test.iterrows():\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Row {idx}:\")\n",
    "    display(HTML(row['html_content']))\n",
    "    # Display the cleaned text\n",
    "    print(\"\\nCleaned Text:\\n\")\n",
    "    print(row['clean_content'].replace('\\n\\n', '\\n---PARAGRAPH BREAK---\\n'))\n",
    "    print(100 * \"-\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "id": "7u1rxL4hdPWl"
   },
   "source": [
    "The double newlines between header and text are not optimal. Additionally the bullet point lists are not captured as such but. Improvements would be possible but we decided to try if Docling handles the conversion already better with the predefined settings.\n",
    "\n",
    "Another thing to note is parts of the texts don't give any useful information, such as the \"Subscribe to Newsletter\" and the \"Staffnet\" chapter or endings such as the following one: \"externe Seite 10.1002/smj.2221 call_made\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "mJSZk1ovOP5T"
   },
   "source": [
    "#### Application of cleaning function on full Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "oeIDmUX2ejoL"
   },
   "source": [
    "Below we apply the conversion to all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "id": "g4_9-XHSN-ON"
   },
   "outputs": [],
   "source": [
    "df['bs_html_title'], df['bs_html_content'] = zip(*df['html_content'].apply(clean_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "q1RAxN_kRMFw",
    "outputId": "5601aea9-630d-4c79-a6bf-689d05b09c8a"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df[['html_content', 'bs_html_title', 'bs_html_content']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {
    "id": "sOwWXwtdTsEc"
   },
   "source": [
    "#### Saving the data to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "id": "2Sjrq6e1TsEc"
   },
   "outputs": [],
   "source": [
    "# Saving the Dataframe to the Google Drive storage\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-02-bs.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {
    "id": "iw-zHzLdVv3k"
   },
   "source": [
    "### Docling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "id": "LcUMgVGixFH7"
   },
   "source": [
    "#### Definition of the Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "id": "HyLTXKhRD9vq"
   },
   "outputs": [],
   "source": [
    "# Create AcceleratorOptions for CUDA\n",
    "cuda_accelerator_options = AcceleratorOptions(device=AcceleratorDevice.CUDA)\n",
    "InputFormat.HTML: HTMLFormatOption(pipeline_options=PipelineOptions(accelerator_options=cuda_accelerator_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "id": "FuACgxzXGSWT"
   },
   "outputs": [],
   "source": [
    "# Initialize the Docling converter\n",
    "converter = DocumentConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "id": "lAy7UjJHYYO4"
   },
   "outputs": [],
   "source": [
    "# Define function for conversion\n",
    "def html_file_to_markdown(file_path):\n",
    "    \"\"\"Convert an HTML file to markdown using Docling\"\"\"\n",
    "    try:\n",
    "        # Convert the HTML file directly by path\n",
    "        result = converter.convert(file_path)\n",
    "        return result.document.export_to_markdown()\n",
    "    except Exception as e:\n",
    "        return f\"Error converting file {file_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "id": "63JM1az1xKBT"
   },
   "source": [
    "#### Application of conversion on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3hBmSg9mYumH",
    "outputId": "f3704c69-5185-437e-ddc9-1f495a8762e5"
   },
   "outputs": [],
   "source": [
    "# Apply the conversion function using the 'full_path' column\n",
    "df_test['markdown_content'] = df_test['full_path'].apply(html_file_to_markdown)\n",
    "\n",
    "# View the result\n",
    "df_test[['html_content', 'title', 'markdown_content']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage/image.imageformat.text50percent.167220564.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage_82956239/image.imageformat.text50percent.342924418.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/en/news-and-events/eth-news/news/2021/06/were-all-greeks/_jcr_content/news_content/textimage_1347267382/image.imageformat.text50percent.2137756140.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "id": "twRFpr_9fDaG",
    "outputId": "8ccf6cb6-0729-4651-b930-5ea2d5fefd90"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "for idx, row in df_test.iterrows():\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Row {idx}:\")\n",
    "    display(HTML(row['html_content']))\n",
    "    # Display the cleaned text\n",
    "    print(\"\\nCleaned Text (Markdown):\\n\")\n",
    "    print(row['markdown_content'])\n",
    "    print(100 * \"-\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "id": "_B5Los74lcFc"
   },
   "source": [
    "**There seems to be a problem with the conversion of Docling. The part of the HTML before the first heading is not included in the Markdown.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "id": "xh1Uo7V5xTAr"
   },
   "source": [
    "#### Application of Conversion on full Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {
    "id": "ZL8j4IsWxeM6"
   },
   "source": [
    "Below we apply the conversion to all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gD8DIQ8ixeM6",
    "outputId": "335cb69e-8974-4832-da3b-480963848082"
   },
   "outputs": [],
   "source": [
    "df['markdown_content_docling'] = df['full_path'].progress_apply(html_file_to_markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "JpvV4QuExeM6",
    "outputId": "87472288-d3a3-4e66-af7b-5bdb509c18ff"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df[['html_content', 'markdown_content_docling']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {
    "id": "LTbt1z2kxeM6"
   },
   "source": [
    "#### Saving the data to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "id": "F8ryTJZMxeM6"
   },
   "outputs": [],
   "source": [
    "# Saving the Dataframe to the Google Drive storage\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-03-docling.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "id": "EfL1PXm4IO86"
   },
   "outputs": [],
   "source": [
    "# For loading\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-03-docling.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {
    "id": "6FyOFaSIlyZ9"
   },
   "source": [
    "### Hybrid Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {
    "id": "VokpeqBLIgOd"
   },
   "source": [
    "#### Definition of the Function for Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "id": "zvs5hMh3vqZA"
   },
   "outputs": [],
   "source": [
    "def html_file_to_markdown_bs_docling(file_path):\n",
    "    \"\"\"Convert an HTML file to markdown, handling content before first header separately\"\"\"\n",
    "    try:\n",
    "        # Read the original HTML file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find the first header\n",
    "        first_header = soup.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "        # If no header is found, just use Docling for the whole document\n",
    "        if not first_header:\n",
    "            result = converter.convert(file_path)\n",
    "            return result.document.export_to_markdown()\n",
    "\n",
    "        # Get the header text to use as a marker\n",
    "        header_text = first_header.get_text().strip()\n",
    "        header_level = int(first_header.name[1])\n",
    "        header_markdown = '#' * header_level + ' ' + header_text\n",
    "\n",
    "        # Process the entire document with Docling\n",
    "        result = converter.convert(file_path)\n",
    "        full_markdown = result.document.export_to_markdown()\n",
    "\n",
    "        # Find where our header appears in the markdown\n",
    "        header_index = full_markdown.find(header_markdown)\n",
    "\n",
    "        # If the header is not found in the markdown, try finding just the header text\n",
    "        if header_index == -1:\n",
    "            header_index = full_markdown.find(header_text)\n",
    "            if header_index == -1:\n",
    "                # If we still can't find it, return the full markdown\n",
    "                return full_markdown\n",
    "\n",
    "        # Extract introduction paragraphs from HTML\n",
    "        intro_paragraphs = []\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            # Only consider paragraphs that appear before the first header\n",
    "            if (\n",
    "                hasattr(paragraph, 'sourceline') and\n",
    "                hasattr(first_header, 'sourceline') and\n",
    "                paragraph.sourceline < first_header.sourceline\n",
    "            ):\n",
    "                text = paragraph.get_text(strip=True)\n",
    "                if text:\n",
    "                    intro_paragraphs.append(text)\n",
    "\n",
    "        # If there are intro paragraphs, combine them\n",
    "        if intro_paragraphs:\n",
    "            intro_markdown = \"\\n\\n\".join(intro_paragraphs)\n",
    "\n",
    "            # Check if the intro is already in the markdown before the header\n",
    "            markdown_before_header = full_markdown[:header_index].strip()\n",
    "\n",
    "            # If intro is already included (or partially included), use the full markdown\n",
    "            if intro_markdown in markdown_before_header or any(p in markdown_before_header for p in intro_paragraphs):\n",
    "                return full_markdown\n",
    "\n",
    "            # Otherwise, add the intro before the header section\n",
    "            return intro_markdown + \"\\n\\n\" + full_markdown[header_index:]\n",
    "        else:\n",
    "            # No intro paragraphs, return the full markdown\n",
    "            return full_markdown\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error converting file {file_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {
    "id": "d8EfsvTLInOw"
   },
   "source": [
    "#### Application of conversion on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "eMloBKjqlmYR",
    "outputId": "d1316ee3-c749-4805-9223-a2de4e762123"
   },
   "outputs": [],
   "source": [
    "# Apply the conversion function using the 'full_path' column\n",
    "df_test['markdown_content_hybrid'] = df_test['full_path'].apply(html_file_to_markdown_bs_docling)\n",
    "\n",
    "# View the result\n",
    "df_test[['html_content', 'full_path', 'markdown_content_hybrid']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage/image.imageformat.text50percent.167220564.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/de/news-und-veranstaltungen/eth-news/news/2021/07/fliegen-daten-und-sieben-velos/_jcr_content/news_content/textimage_82956239/image.imageformat.text50percent.342924418.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      },
      "http://localhost:8080/en/news-and-events/eth-news/news/2021/06/were-all-greeks/_jcr_content/news_content/textimage_1347267382/image.imageformat.text50percent.2137756140.jpg": {
       "data": "",
       "headers": [
        [
         "content-length",
         "0"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "id": "ChKQdho2tUAW",
    "outputId": "9dadd223-f199-4aad-9c19-3a61766fe030"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "for idx, row in df_test.iterrows():\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"Row {idx}:\")\n",
    "    display(HTML(row['html_content']))\n",
    "    # Display the cleaned text\n",
    "    print(\"\\nCleaned Text (Markdown Hybrid):\\n\")\n",
    "    print(row['markdown_content_hybrid'])\n",
    "    print(100 * \"-\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {
    "id": "071vrAy7GScP"
   },
   "source": [
    "#### Application of Conversion on full Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {
    "id": "N0P2pS2cGScP"
   },
   "source": [
    "Below we apply the conversion to all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GCjr_FK8GScP",
    "outputId": "344f1519-571b-498c-fb72-23ab20614872"
   },
   "outputs": [],
   "source": [
    "df['markdown_content_hybrid'] = df['full_path'].progress_apply(html_file_to_markdown_bs_docling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QjgKzIkOGScP",
    "outputId": "a3a559df-d7db-4422-d364-b097990f2537"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 150)\n",
    "df[['html_content', 'markdown_content_hybrid']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {
    "id": "KWS4MnT8GScQ"
   },
   "source": [
    "#### Saving the data to storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "id": "fkS9H_zjGScQ"
   },
   "outputs": [],
   "source": [
    "# Saving the Dataframe to the Google Drive storage\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-04-hybrid.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {
    "id": "3N0HTlOUd23Z"
   },
   "source": [
    "# Multilingual Text Preprocessing and Cleaning (5 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "id": "oXsVInydIW7F"
   },
   "outputs": [],
   "source": [
    "# For loading\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-04-hybrid.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "id": "n7T6YlIjzSlt"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Perform necessary text preprocessing (e.g., removing extra spaces and redundant line breaks, normalizing\n",
    "Unicode characters, standardizing date formats from different sources), and handle German-specific text\n",
    "processing (e.g., compound words, umlaut normalization if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "6y7iTq5l1mlz",
    "outputId": "3e4cd0e0-a218-4bc0-cb05-b70d9b3cf422"
   },
   "outputs": [],
   "source": [
    "df[\"markdown_content_hybrid\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {
    "id": "b5Y6ROSgy6uK"
   },
   "source": [
    "## Metadata\n",
    "\n",
    "Store the cleaned text and its metadata in a structured format suitable for retrieval (e.g., JSON, CSV, or a\n",
    "database) with fields such as **language, title, date, source**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {
    "id": "cxcfzKiAeJqy"
   },
   "source": [
    "The following step enriches the dataframe by adding the date (month and year) extracted from the file path, as they are organized by date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "id": "t3ccy0U_cxlp"
   },
   "outputs": [],
   "source": [
    "# Function to extract year and month from the folder path\n",
    "def extract_year_month(path):\n",
    "\n",
    "    if isinstance(path, str):  # Check if path is a string.\n",
    "        parts = path.split('/')\n",
    "        if len(parts) >= 2: # Check if the path has at least two parts\n",
    "            month = parts[-1]\n",
    "            year = parts[-2]\n",
    "            return year, month\n",
    "        else:\n",
    "             return None, None\n",
    "    else:\n",
    "        return None, None #Handles the case where the input is not a string\n",
    "\n",
    "# Apply the function to create new columns 'year' and 'month'\n",
    "df[['year', 'month']] = df['folder_path'].apply(lambda x: pd.Series(extract_year_month(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {
    "id": "l8f9A7sKiSh8"
   },
   "source": [
    "Extracts the type of document title and the language from the folder path structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "id": "IDJYB_nKhymZ"
   },
   "outputs": [],
   "source": [
    "def extract_language_type(path):\n",
    "    if isinstance(path, str):\n",
    "        parts = path.split('/')\n",
    "        if len(parts) >= 4:  # Check for the third element from the end\n",
    "            third_from_end = parts[-3]\n",
    "            lang_type_parts = third_from_end.split('_')\n",
    "            language = lang_type_parts[0] if lang_type_parts[0] in ('de', 'en') else None\n",
    "            Type = 'internal' if len(lang_type_parts) > 1 and lang_type_parts[1] == 'internal' else \\\n",
    "                   'news events' if len(lang_type_parts) > 1 and lang_type_parts[1] == 'news' else None\n",
    "            return language, Type\n",
    "        else:\n",
    "            return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to create new columns 'language' and 'Type'\n",
    "df[['language', 'type']] = df['folder_path'].apply(lambda x: pd.Series(extract_language_type(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "id": "jbQnQ4C_eyHB"
   },
   "source": [
    "Extracts HTML file name and adds it to the dataframe. This could be useful as a backup title, as many files do not contain a `<meta>` tag with the title or a `<h1>` tag for the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "id": "No2TT5HLfEuS"
   },
   "outputs": [],
   "source": [
    "# Function to extract and format the title from the file_name\n",
    "def extract_and_format_title(file_name):\n",
    "    \"\"\"\n",
    "    Extracts the title from the filename, removes the '.html' extension,\n",
    "    replaces hyphens with spaces, and capitalizes only the first letter of the first word.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The filename.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted title, or None if the input is not a string.\n",
    "    \"\"\"\n",
    "    if isinstance(file_name, str):\n",
    "        title = file_name.replace(\".html\", \"\").replace(\"-\", \" \")\n",
    "        words = title.split()\n",
    "        if words:\n",
    "            words[0] = words[0].capitalize()\n",
    "            title = \" \".join(words)\n",
    "        return title\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'html_title' column\n",
    "df['html_title'] = df['file_name'].apply(extract_and_format_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZLLSJ4nveuDq",
    "outputId": "fa7711c0-510f-479b-d3ff-89cef6ebc40a"
   },
   "outputs": [],
   "source": [
    "# Print the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2yhjSn9i4Ej",
    "outputId": "4f035589-d0f3-4509-b146-ef198833382b"
   },
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "# drop 'bs_html_content'and 'markdown_content_docling' columns\n",
    "df1 = df.drop(columns=['bs_html_content', 'markdown_content_docling'])\n",
    "print(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "id": "X9jtVuuImHMK"
   },
   "outputs": [],
   "source": [
    "# save file as csv\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-05-metadata.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {
    "id": "JTw10dtvGzAI"
   },
   "source": [
    "## Metadata Extraction from Content (NLP)\n",
    "\n",
    "In the following steps, information about the text will be extracted and added as metadata to the dataframe, fields such as **main content, named entities, topics, keywords, summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {
    "id": "kTpcUPvaFPAJ"
   },
   "outputs": [],
   "source": [
    "# load step 5 dataset\n",
    "pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-05-metadata.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {
    "id": "gaYfTXlxFbgQ"
   },
   "source": [
    "Performs necessary text preprocessing (e.g., removing extra spaces and redundant line breaks, normalizing\n",
    "Unicode characters, and handle German-specific text\n",
    "processing 'ß'.\n",
    "\n",
    "We decided that there is no need to process and unify date structure as the date is already stracted from the path of the files. We might lose some information regarding the day if included in some texts, but with a span of over a decade, that level of granularity wont be necessary.\n",
    "\n",
    "Regarding the German specific processing such as umlaut or compound words:\n",
    "- After normalizing unicode characters, we can see that umlauts and other characters are not an issue and are displayed proyerly, so there is no need for further processing.\n",
    "- Compound words are an essential part of the German language, and processing it could affect negatively the meaning and understandability of the text explained, as explained by our german speaking colleague Pascal, hence we decided against of processing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {
    "id": "m7N0q1HuFU-g"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#print an example of the text in the markdown_content_hybrid column\n",
    "print(df['markdown_content_hybrid'].iloc[0])\n",
    "\n",
    "def preprocess_text(text):\n",
    "\t# Remove extra spaces and redundant line breaks\n",
    "\ttext = re.sub(r'\\s+', ' ', text)\n",
    "\ttext = text.strip()\n",
    "\n",
    "\t# Normalize Unicode characters (if needed)\n",
    "\ttext = text.encode('utf-8').decode('utf-8')\n",
    "\n",
    "\t# Handle German-specific text processing (e.g., compound words, umlaut normalization)\n",
    "\ttext = text.replace('ß', 'ss')\n",
    "\n",
    "\treturn text\n",
    "\n",
    "# Apply the preprocessing function to the 'markdown_content_hybrid' column\n",
    "df['content'] = df['markdown_content_hybrid'].progress_apply(preprocess_text)\n",
    "\n",
    "print(df['content'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "id": "mEPZQprdMthj"
   },
   "source": [
    "The next step processes the (`content` column) and its language (`language` column)to perform the following tasks:\n",
    "\n",
    "1. **Loads NLP Models**: Initializes spaCy models for English and German text processing.\n",
    "2. **Extracts Features**: Defines a function `extract_all` to extract:\n",
    "   - Named entities using spaCy.\n",
    "   - Keywords using KeyBERT.\n",
    "   - Topics using Gensim's LDA.\n",
    "   - A summary (heuristic: two longest sentences).\n",
    "3. **Applies the Function**: Processes each row of the DataFrame using `progress_apply` and adds the extracted features as new columns (`named_entities`, `topics`, `keywords`, `summary`).\n",
    "4. **Displays Results**: Prints the first few rows of the updated DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "id": "scbY2GQVH2Z-"
   },
   "outputs": [],
   "source": [
    "# --- Install required packages if needed ---\n",
    "# pip install spacy keybert gensim tqdm\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download de_core_news_sm\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from keybert import KeyBERT\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Init KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Enable progress_apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# --- Define the extraction function ---\n",
    "def extract_all(text, lang):\n",
    "    # Select language-specific spaCy model\n",
    "    if lang == \"de\":\n",
    "        nlp = nlp_de\n",
    "    else:\n",
    "        nlp = nlp_en\n",
    "\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Named Entities\n",
    "    entities = list(set((ent.text, ent.label_) for ent in doc.ents))\n",
    "\n",
    "    # Keywords\n",
    "    keywords = kw_model.extract_keywords(text, top_n=5)\n",
    "    keywords = [kw[0] for kw in keywords]\n",
    "\n",
    "    # Topics using LDA (via Gensim)\n",
    "    tokens = [token.text.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    try:\n",
    "        lda_model = models.LdaModel(corpus, num_topics=1, id2word=dictionary, passes=4)\n",
    "        topics = [word for word, _ in lda_model.show_topic(0)]\n",
    "    except:\n",
    "        topics = []\n",
    "\n",
    "    # Summary (heuristic: longest 2 sentences)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 20]\n",
    "    sentences = sorted(sentences, key=lambda s: len(s), reverse=True)\n",
    "    summary = \" \".join(sentences[:2]) if sentences else \"\"\n",
    "\n",
    "    return pd.Series({\n",
    "        \"named_entities\": entities,\n",
    "        \"topics\": topics,\n",
    "        \"keywords\": keywords,\n",
    "        \"summary\": summary\n",
    "    })\n",
    "\n",
    "# --- Apply it to your DataFrame ---\n",
    "# Make sure your df has 'content' and 'language' columns\n",
    "df = df.dropna(subset=[\"content\", \"language\"])\n",
    "df[[\"named_entities\", \"topics\", \"keywords\", \"summary\"]] = df.progress_apply(\n",
    "    lambda row: extract_all(row[\"content\"], row[\"language\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: display result\n",
    "print(df[[\"content\", \"language\", \"named_entities\", \"topics\", \"keywords\", \"summary\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {
    "id": "8jEeSXjyH3vA"
   },
   "outputs": [],
   "source": [
    "# save the updated dataframe to a new CSV file\n",
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-06-NLP-processed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {
    "id": "FXSCE24rRKfa"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-06-NLP-processed.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {
    "id": "QiD9PBJSavb3"
   },
   "source": [
    "### Evaluation of Summary\n",
    "\n",
    "This code uses `sumy` library to extract the summary from the `content` using Latent Semantic Analysis (LSA). Tt processes the text content, applies the `LsaSummarizer` to extract key sentences based on topic relevance, and removes stop words to enhance readability. The summarization function is then applied to each row in the DataFrame, creating a new column with summarized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "id": "XWoUlESFRzai"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sumy.parsers.plaintext import PlaintextParser  # Corrected import\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Function to summarize text based on language\n",
    "i=0\n",
    "def summarize_text(row):\n",
    "    global i\n",
    "    language = row[\"language\"]\n",
    "    parser = PlaintextParser.from_string(row[\"content\"], Tokenizer(language))  # Corrected usage\n",
    "    summarizer = LsaSummarizer()\n",
    "    summarizer.stop_words = get_stop_words(language)\n",
    "\n",
    "    summary_sentences = summarizer(parser.document, 2)  # Get top 2 sentences\n",
    "    i=i+1\n",
    "    print(i)\n",
    "    return \" \".join([str(sentence) for sentence in summary_sentences])\n",
    "\n",
    "# Apply summarization\n",
    "df[\"summary_summy\"] = df.apply(summarize_text, axis=1)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {
    "id": "vWx857BpZ_WE"
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-07-summaryv2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c9iCtFh7UZa",
    "outputId": "4e13b51d-fc6c-4897-9c4a-138daa609d8d"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-07-summaryv2.csv'))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnHRsZN37bak",
    "outputId": "dc1ba748-7828-486c-b6cb-7b97e71f4194"
   },
   "outputs": [],
   "source": [
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730,
     "referenced_widgets": [
      "d1f9eb383c0e492eadb402dac58e4013",
      "92a225b20887486797c0f04c88392f11",
      "002b74df2dff45bca28aa2ab307581ef",
      "e3a05b5784734595b4b85de057658806",
      "bbd967e48424417a8bb165d350ebb82c",
      "ab74d223da0c4e6d927534c0b4f1b3af",
      "746a829e861b45769b52f31a28b386aa",
      "83694e0009784ef29a6b7e4422666c1b",
      "0acd2ce3046e4ddf8a00aa5bde3eba53",
      "c24f1ab4d5a64c5da510884f17a72e0e",
      "e03ac720617f48349ecd65601e3665d9",
      "37195a5ac75e4cda8da89ac7f799ca01",
      "6484bd0c22b74dc5939064fb0e1a0603",
      "5fff99348b7d480d91f7ad60a6f2006c",
      "f4299be273d844eebb6b78b01db07d1e",
      "99435a77cd2343f6bfbc8936246fcf27",
      "acb11e001df143089e5d2da0bd3ad14c",
      "8e7759619d044d3bb4f1e5828bf80385",
      "279aadffe9874b698c3afb3daaf050d1",
      "d4897c3d13294f43b572a90ce3809e7d",
      "5d19b802ef294100bb6b3bb372459d0c",
      "eb1c7c7ef30c45bb9c0383bcb6acd42f",
      "c5610826c75845eca08ae61835273d0b",
      "33d0152eace84ccf92d91572b6a9ccdf",
      "e46a05d092fc4c829cc98042d3cd46d9",
      "ea0293644f91426887d10ed93d8eaca5",
      "7c644b1c80bc4f94b64f86d8cf673995",
      "bb3540c006d44d8a9016a65511cd61ce",
      "480e983f14204f509bef1d75531b8d0c",
      "21dd9bd87d874659a6779959f9191f77",
      "e50afbaa275448a89faf9c5596bc3650",
      "9a5351e35ac54770b6abbc29c220d410",
      "c1c80688c1dc46529451422f1b2ac649",
      "e1bebff9fc8245b79a50b62bba6b6905",
      "8e6b2e36e3f64c1aa72567b749d2571b",
      "480b993f2af449979499b9a237f238f5",
      "3bbb266e5eef457e9d902e6e59143be5",
      "6ea7cfefd0d14229a0cf4a3ed67b160f",
      "1180a35c008448e9b8a29fdd7bc6f347",
      "c691fd7e62624853b718b091c5183e2c",
      "0a095a8e694440919a8a254f50a2deb0",
      "7511b424fd414201800e485a0a90d136",
      "fadcb8cdf0cf48d3882f2389a046876f",
      "2f5e469597a54e859317a96e2c05d024",
      "f94e45a97a77468a83868f4bd9793bed",
      "8844880b329a4dd19b600b7ade6486a5",
      "1a8460c3943d4961821ce903e1a93077",
      "3e204adb1a6043c6a95cf54c62d4c3a4",
      "2834dfb7be53435bbfa3270a6a66bb4a",
      "da094a8fdb534befa5701855a4898193",
      "a16df48e941a4cc3b5bea6a0487abd67",
      "c39402a3f11840ddbee9161e9bb7f435",
      "feec0560ba5149a194ced0e53133478e",
      "18732b61970544309607ba1663e90857",
      "628ab44839024180a608cb9f22796ebd",
      "437a8c094176476eb03b022593417f5c",
      "5fd84d97326f47659e501d8cfad4843a",
      "3ef024184bb949e4a9c907688194695b",
      "ec145face28b49eca85b426a607520b3",
      "de49fc70de6244ed89c996d5ef160828",
      "4bf7cb9e82d74937824ff6b7762ef05d",
      "e88a523bfd4440fca2c9d7c229a8cc71",
      "8019d11cd9e6464e9e5b8eefe42f52e2",
      "646598a20fa4443fad3f2354f5dd5472",
      "020c4db04ede43a38a230d76e9a695f2",
      "ff0f57f986db4dc5a8c037c126fc0701",
      "c3a8718bfc7848109dc0779d5882e1d1",
      "333c9e4bff1d4c4ba9ee222b87bcd38b",
      "84871f0c0cb04e118a1e8c2797262964",
      "96b2358b21b04ff0b767028e462a0aa0",
      "b628370e6d0d447380e8536281d8b9e4",
      "27cf99e15d0746fc80c1c6c68bb676c0",
      "872dab076f4c42c0bae6410f6f425ed9",
      "f81b11d2a0394b8098e18ea61a3d59f1",
      "f8e2040b6a58461fb60daeb2e75c0475",
      "7ac59349592e4eafa3034ba99b0e4176",
      "7c45b2c742804bb7ac51360ccaaf9b58",
      "cf266e5805b84efa82726547f4cb6a9d",
      "ea32e1856f17437984a5c86583c570f1",
      "7287b35f6c504cfcac3ace5e7f5d8205",
      "ccbf79e0bec243ba8033c2277b891255",
      "f73f501c8a864a2c84eaf69e30640ce6",
      "bf8c69d0854c4f8999292d111aa269ae",
      "506fd6bc80374969b9fe501f5e66ca8f",
      "5444d179779b47aea1cb31fbd8d7e9dd",
      "e41dc9ea281343c6a09f76e1adf1c9a8",
      "623ee61fcb924ebc95e9c9e380f9d718",
      "0e9a6206d01c4a2a817fda708b1134b5",
      "a64b38e83b854bd7872fe2bb9eb59d51",
      "def01702fd7d43158fd0b4514df75ebb",
      "21f8116104d249f5ac7cb44d72f1bd00",
      "24f249b858334bc896f47a65d5accb76",
      "f1b48783d3cb43838046d75f901ce171",
      "7f0225f1e9ab46ca819f1e8057849397",
      "015813ce656f48d79afab73b5b5ae73e",
      "3c1157e8b704437fbd276fa78d0d1fee",
      "5b54d330a39d41fa9b26612ea45cd097",
      "438829ae1448499691748452554ff2cf",
      "4457c930a24d4a55bd17e902120ab0f4",
      "270ada376453468ea6cd3f6467b52ab5",
      "a14d1455878b4b15adf478631e83861c",
      "90f3e106804049608d6e29a4784d7c86",
      "0fd23002cdf145868796d278529f0708",
      "26d86cb934f84437809490e7a4202919",
      "e8151ae875354d87af6dccdca6fdf8ee",
      "5222c110fbf24e2a9c6c801a096f2d44",
      "14ec5ad5862542048d8b4c4b38d4da72",
      "9e47a7734de34cc1a8664ddddf7b26c0",
      "4b14187b70614d1fbe4b459b388ee15a",
      "1b8570724b3a494bb4a913d8c1906d66"
     ]
    },
    "id": "MdBELGPJ7x0t",
    "outputId": "da5e2cac-077b-46a2-a0c1-1d4154d8e0e7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bert_score import score\n",
    "\n",
    "# Make sure content and summaries are strings\n",
    "df = df.dropna(subset=['content', 'summary', 'summary_summy'])\n",
    "contents = df['content'].astype(str).tolist()\n",
    "summaries_1 = df['summary'].astype(str).tolist()\n",
    "summaries_2 = df['summary_summy'].astype(str).tolist()\n",
    "\n",
    "# Compute BERTScore (Precision, Recall, F1) for both sets of summaries\n",
    "P1, R1, F1 = score(summaries_1, contents, lang=\"en\", verbose=True)\n",
    "P2, R2, F2 = score(summaries_2, contents, lang=\"en\", verbose=True)\n",
    "\n",
    "# Add results to DataFrame\n",
    "df['bertscore_summary_f1'] = F1.tolist()\n",
    "df['bertscore_summy_f1'] = F2.tolist()\n",
    "\n",
    "# Compare average F1 scores\n",
    "mean_f1_summary = F1.mean().item()\n",
    "mean_f1_summy = F2.mean().item()\n",
    "\n",
    "print(f\"Mean BERTScore F1 - summary:      {mean_f1_summary:.4f}\")\n",
    "print(f\"Mean BERTScore F1 - summary_summy: {mean_f1_summy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {
    "id": "GXiEM79O9xqS"
   },
   "source": [
    "The result of comparing both summaries ends in a tie:\n",
    "- Mean BERTScore F1 - summary:      0.8648\n",
    "- Mean BERTScore F1 - summary_summy: 0.8640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXaU6Nj691Yi",
    "outputId": "a39e8abd-d49c-4b8e-cb91-614fbe484070"
   },
   "outputs": [],
   "source": [
    "# filter the dataframe by 'language' = 'en' and take 10 random samples with a fixed seed\n",
    "sample = df[df['language'] == 'en'].sample(10, random_state=42)[['summary', 'summary_summy']]\n",
    "\n",
    "# print the samples in pairs\n",
    "for i, row in sample.iterrows():\n",
    "\tprint(f\"Summary: {row['summary']}\")\n",
    "\tprint(f\"Summy: {row['summary_summy']}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {
    "id": "DtuQChBIAPkB"
   },
   "source": [
    "The heuristic-based approach selects the longest sentences, assuming they are the most relevant, but often fails to capture the true essence of the content, producing long chunks without contextual analysis.\n",
    "\n",
    "In contrast, summy uses advanced algorithms to extract meaningful sentences, creating concise and focused summaries based on the data. When summaries differ, summy consistently provides more context-aware and coherent outputs by inferring importance rather than relying on sentence length.\n",
    "\n",
    "While both methods may achieve similar BERT scores, summy demonstrates a deeper understanding of the content, producing summaries that are both accurate and contextually meaningful.\n",
    "\n",
    "Hence we will keep only the sumy summary and delete the helper columns on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Og4lJxO3CBNB",
    "outputId": "c3fcb83d-e973-4089-fdbe-4719877cd5e8"
   },
   "outputs": [],
   "source": [
    "# Load again the dataset to avoid deleting several columns created in the previous steps\n",
    "df = pd.read_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-07-summaryv2.csv'))\n",
    "\n",
    "# Delete 'summary' column\n",
    "df.drop(columns=['summary'], inplace=True)\n",
    "\n",
    "# rename column 'summary_summy' to summary\n",
    "df.rename(columns={'summary_summy': 'summary'}, inplace=True)\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {
    "id": "7HBf9YewCrTT"
   },
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(base_folder, 'Stage1/Working-dir/Stage1-08-final.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
